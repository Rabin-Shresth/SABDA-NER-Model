 

@article{1,
author = {Bam, Surya and Shahi, Tej},
year = {2014},
month = {01},
pages = {21-29},
title = {Named Entity Recognition for Nepali Text Using Support Vector Machines},
volume = {06},
journal = {Intelligent Information Management},
doi = {10.4236/iim.2014.62004}
}

@article{2,
author = {Arindam Dey, Abhijit Paul, Bipul Syam Purkayastha},
year = {2014
},
month = {02},
pages = {24-25},
title = {Named Entity Recognition for Nepali language: A
Semi Hybrid Approach
},
volume = {03},
journal = {International Journal of Engineering and Innovative Technology (IJEIT)},
doi = { }
}

@article{3,
author = {Singh, Oyesh and Padia, Ankur and Joshi, Anupam},
year = {2019},
month = {08},
pages = {},
title = {Named Entity Recognition for Nepali Language}
}

 


@misc{displaCy,
  author      = {Ines Montani},
  title       = {displaCy},
  howpublished = {{\url{https://spacy.io/universe/project/displacy}}},
  year        = { },
  note        = { },
}


@article{Niraula_Chapagain_2023, title={DanfeNER - Named Entity Recognition in Nepali Tweets}, volume={36}, url={https://journals.flvc.org/FLAIRS/article/view/133384}, DOI={10.32473/flairs.36.133384}, abstractNote={&amp;lt;p&amp;gt;Twitter allows users to easily post tweets on any subject or event anytime, generating massive amounts of rich text content on diverse topics. Automated methods such as Named Entity Recognition (NER) are required to process the massive tweet data. Processing tweets, however, poses a special challenge as they are informal posts with incomplete context and often contain acronyms, hashtags, misspellings, abbreviations, and URLs due to length constraints. This paper presents the first systematic study of NER in Nepali tweets corresponding to five different entity types: Person Name (PER), Location (LOC), Organization (ORG), Date (DAT), and Event (EVT). We develop DanfeNER, the first human-labeled high-quality NER benchmark data sets for the low-resource language Nepali. DanfeNER contains 5,366 records and 3,463 entities in its train set and 2,301 records and 1,503 entities in its test set. Using this data set, we benchmark several state-of-the-art Nepali monolingual and multilingual transformer models, obtaining micro-averaged F1 scores up to 81%.&amp;lt;/p&amp;gt;}, number={1}, journal={The International FLAIRS Conference Proceedings}, author={Niraula, Nobal and Chapagain, Jeevan}, year={2023}, month={May} }




@misc{Agile,
  author      = {Pekka Abrahamsson, Outi Salo, Jussi Ronkainen, Juhani Warsta},
  title       = {Agile Software Development Methods: Review and Analysis},
  howpublished = {{\url{https://https://arxiv.org/abs/1709.08439}}},
  year        = {25 Sep 2017 },
  note        = { },
}

@misc{colab,
  author      = { google},
  title       = { Google Colaboratory},
  howpublished = {{\url{//https://colab.research.google.com/}}},
  year        = {  },
  note        = { },
}


@misc{HuggingFace,
  author      = {  },
  title       = { HuggingFace/datasets},
  howpublished = {{\url{//https://huggingface.co/datasets}}},
  year        = {  },
  note        = { },
}


@misc{seqeval,
  title={{seqeval}: A Python framework for sequence labeling evaluation},
  url={https://github.com/chakki-works/seqeval},
  note={Software available from https://github.com/chakki-works/seqeval},
  author={Hiroki Nakayama},
  year={2018},
}


@misc{W.,
  author      = {W. &. Biases  },
  title       = { Weights & Biases},
  howpublished = {{\url{ https://docs.wandb.ai/}}},
  year        = {  },
  note        = { },
}

@misc{ attention,
  author      = {Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin  },
  title       = {Attention Is All You Need},
  howpublished = {{\url{https://arxiv.org/abs/1706.03762}}},
  year        = { 12 Jun 2017 },
  note        = { },
}


@misc{ Transformer?,
  author      = {Maxime },
  title       = {What is a Transformer?},
  howpublished = {{\url{https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04}}},
  year        = {Jan 4, 2019 },
  note        = { },
}

@misc{BERT,
  author      = {Marcello Politi},
  title       = {Custom Named Entity Recognition with BERT},
  howpublished = {\url{https://towardsdatascience.com/custom-named-entity-recognition-with-bert-cf1fd4510804}},
  year        = {Jul 8, 2022 },
  note        = { },
}


@misc{mhaske2022naamapadam,
  doi = {10.48550/ARXIV.2212.10168},
  url = {https://arxiv.org/abs/2212.10168},
  author = {Mhaske, Arnav and Kedia, Harshit and Doddapaneni, Sumanth and Khapra, Mitesh M. and Kumar, Pratyush and Murthy, Rudra and Kunchukuttan, Anoop},
  title = {Naamapadam: A Large-Scale Named Entity 
             Annotated Data for Indic Languages},
  publisher  = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{litake2022mono,
  title={Mono vs multilingual BERT: A case study in hindi and marathi named entity recognition},
  author={Litake, Onkar and Sabane, Maithili and Patil, Parth and Ranade, Aparna and Joshi, Raviraj},
  journal={arXiv preprint arXiv:2203.12907},
  year={2022}
}

@inproceedings{hakala-pyysalo-2019-biomedical,
    title = "Biomedical Named Entity Recognition with Multilingual {BERT}",
    author = "Hakala, Kai  and
      Pyysalo, Sampo",
    editor = "Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger",
    booktitle = "Proceedings of the 5th Workshop on BioNLP Open Shared Tasks",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5709",
    doi = "10.18653/v1/D19-5709",
    pages = "56--61",
    abstract = "We present the approach of the Turku NLP group to the PharmaCoNER task on Spanish biomedical named entity recognition. We apply a CRF-based baseline approach and multilingual BERT to the task, achieving an F-score of 88{\%} on the development data and 87{\%} on the test set with BERT. Our approach reflects a straightforward application of a state-of-the-art multilingual model that is not specifically tailored to either the language nor the application domain. The source code is available at: \url{https://github.com/chaanim/pharmaconer}",
}

@inproceedings{bade-shrestha-bal-2020-named,
    title = "Named-Entity Based Sentiment Analysis of {N}epali News Media Texts",
    author = "Bade Shrestha, Birat  and
      Bal, Bal Krishna",
    editor = "YANG, Erhong  and
      XUN, Endong  and
      ZHANG, Baolin  and
      RAO, Gaoqi",
    booktitle = "Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.nlptea-1.16",
    pages = "114--120",
    abstract = "Due to the general availability, relative abundance and wide diversity of opinions, news Media texts are very good sources for sentiment analysis. However, the major challenge with such texts is the difficulty in aligning the expressed opinions to the concerned political leaders as this entails a non-trivial task of named-entity recognition and anaphora resolution. In this work, our primary focus is on developing a Natural Language Processing (NLP) pipeline involving a robust Named-Entity Recognition followed by Anaphora Resolution and then after alignment of the recognized and resolved named-entities, in this case, political leaders to the correct class of opinions as expressed in the texts. We visualize the popularity of the politicians via the time series graph of positive and negative sentiments as an outcome of the pipeline. We have achieved the performance metrics of the individual components of the pipeline as follows: Part of speech tagging {--} 93.06{\%} (F1-score), Named-Entity Recognition {--} 86{\%} (F1-score), Anaphora Resolution {--} 87.45{\%} (Accuracy), Sentiment Analysis {--} 80.2{\%} (F1-score).",
}

@inproceedings{timilsina-etal-2022-nepberta,
    title = "{N}ep{BERT}a: {N}epali Language Model Trained in a Large Corpus",
    author = "Timilsina, Sulav  and
      Gautam, Milan  and
      Bhattarai, Binod",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-short.34",
    pages = "273--284",
    abstract = "Nepali is a low-resource language with more than 40 million speakers worldwide. It is written in Devnagari script and has rich semantics and complex grammatical structure. To this date, multilingual models such as Multilingual BERT, XLM and XLM-RoBERTa haven{'}t been able to achieve promising results in Nepali NLP tasks, and there does not exist any such a large-scale monolingual corpus. This study presents NepBERTa, a BERT-based Natural Language Understanding (NLU) model trained on the most extensive monolingual Nepali corpus ever. We collected a dataset of 0.8B words from 36 different popular news sites in Nepal and introduced the model. This data set is 3 folds times larger than the previous publicly available corpus. We evaluated the performance of NepBERTa in multiple Nepali-specific NLP tasks, including Named-Entity Recognition, Content Classification, POS Tagging, and Sequence Pair Similarity. We also introduce two different datasets for two new downstream tasks and benchmark four diverse NLU tasks altogether. We bring all these four tasks under the first-ever Nepali Language Understanding Evaluation (Nep-gLUE) benchmark. We will make Nep-gLUE along with the pre-trained model and data sets publicly available for research.",
}

@article{li2020survey,
  title={A survey on deep learning for named entity recognition},
  author={Li, Jing and Sun, Aixin and Han, Jianglei and Li, Chenliang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={34},
  number={1},
  pages={50--70},
  year={2020},
  publisher={IEEE}
}